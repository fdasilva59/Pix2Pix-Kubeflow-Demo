{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pix2Pix : Aerial images to maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Work based on : \"**Image-to-Image Translation with Conditional Adversarial Networks**\" \n",
    "\n",
    "(See [arXiv:1611.07004v3 [cs.CV]](https://arxiv.org/abs/1611.07004) by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou and Alexei A. Efros - [Project Homepage](https://github.com/phillipi/pix2pix) )\n",
    "\n",
    "\n",
    "(NEW) See also https://www.tensorflow.org/alpha/tutorials/generative/pix2pix in Tensorflow 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "REMOTE : Deploy and run a Kubeflow Pipeline from outside the Kubeflow cluster\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prerequisites for this demo:\n",
    "- a Google Cloud Platform (GCP) project with a **IAP-enabled cluster** running on Kubernetes Engine (GKE)\n",
    "- a GCP service account with the necessary permissons, and added as an 'IAP-secured Web App User'\n",
    "\n",
    "Some instructions on how to setup this area available [here](https://github.com/amygdala/examples/blob/cookbook/cookbook/pipelines/notebooks/kfp_remote_deploy.ipynb)\n",
    "\n",
    "In summary, you have to define in your GCP/GKE environment, a Google json key file for the service account deployment on the GKE Kubeflow cluster. To run the notebook locally, you have to set the GOOGLE_APPLICATION_CREDENTIALS environment var to point to your service account credentials:\n",
    "\n",
    "`export GOOGLE_APPLICATION_CREDENTIALS=<your_json_key_file_path> `\n",
    "\n",
    "(Note : you should do this before launching your Jupyter Notebook server)\n",
    "\n",
    "**NOTE** We will reuse *the same* Python code we have used to Build a Pix2Pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/shared/others/pix2pix-map/nextatos-201903-1ef27dfd68c6.json\n"
     ]
    }
   ],
   "source": [
    "# Check the Google JSON key file for the service account deployment on the GKE Kubeflow cluster\n",
    "!echo $GOOGLE_APPLICATION_CREDENTIALS\n",
    "\n",
    "#!cat $GOOGLE_APPLICATION_CREDENTIALS   # It's a secret !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Ready for notebook execution\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Update the environment if needed\n",
    "!pip3 install  matplotlib --quiet --disable-pip-version-check\n",
    "!pip3 install tensorflow==1.12.0 --quiet --disable-pip-version-check --upgrade\n",
    "!pip3 install https://storage.googleapis.com/ml-pipeline/release/0.1.20/kfp.tar.gz --quiet --disable-pip-version-check --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import os\n",
    "\n",
    "# Import PiX2Pix code \n",
    "from utils import *\n",
    "from download_dataset import *\n",
    "from prepare_dataset import *\n",
    "from train_pix2pix import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kubeflow SDK setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Kubeflow Pipelines SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import kfp.notebook\n",
    "import kfp.components as comp\n",
    "from kfp import compiler\n",
    "from kubernetes import client as k8s_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Convert Python Functions into Pipeline operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------- \n",
    "# Convert Python Functions into Pipeline operations \n",
    "#---------------------------------------------\n",
    "\n",
    "download_op = comp.func_to_container_op(download_dataset,\n",
    "                                        base_image='tensorflow/tensorflow:1.12.0-py3' )\n",
    "\n",
    "\n",
    "preparation_op = comp.func_to_container_op(prepare_dataset,\n",
    "                                           base_image='tensorflow/tensorflow:1.12.0-py3' )\n",
    "\n",
    "# Training Component will be executed on a CPU node in this quick demo test\n",
    "training_op = comp.func_to_container_op(train_pix2pix,\n",
    "                                       base_image='tensorflow/tensorflow:1.12.0-gpu-py3' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Build and Compile the pix2pix Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/maps.tar.gz\"\n",
    "FILE_NAME = \"maps.tar.gz\"\n",
    "\n",
    "#--------------------------------------------- \n",
    "#           NFS PATHS on GKE\n",
    "#---------------------------------------------\n",
    "NFS_MOUNT = \"/mnt/nfs\"\n",
    "KERAS_CACHE_DIR = \"/mnt/nfs/data/\"\n",
    "PATH_TO_TFRECORDS = \"/mnt/nfs/data/datasets/{{workflow.name}}\"  \n",
    "PATH_TO_OUTPUTS = \"/mnt/nfs/data/outputs/{{workflow.name}}\"\n",
    "PATH_TO_CHECKPOINTS =\"/mnt/nfs/data/models/{{workflow.name}}\"\n",
    "\n",
    "#--------------------------------------------- \n",
    "#              Google Storage \n",
    "#\n",
    "# (so that Kubeflow pipelines can display \n",
    "#  logs them in the Tensorboard widget)\n",
    "#\n",
    "# IMPORTANT  : CUSTOMIZE this variable with \n",
    "#              your own Bucket URL\n",
    "#\n",
    "#--------------------------------------------- \n",
    "PATH_TO_TF_LOGS = 'gs://wl-tex10-kfp-001/tf-logs/{{workflow.name}}'\n",
    "\n",
    "\n",
    "#--------------------------------------------- \n",
    "#      Build the pix2pix Pipeline Function\n",
    "#---------------------------------------------\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Pix2Pix pipeline',\n",
    "    description='A pipeline to download and prepare the dataset and train Pix2Pix'\n",
    ")\n",
    "def pix2pix(\n",
    "    \n",
    "    ## -- Download Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    origin = dsl.PipelineParam('origin', value=URL),\n",
    "    fname = dsl.PipelineParam('fname', value=FILE_NAME),\n",
    "    cachedir = dsl.PipelineParam('cachedir', value=KERAS_CACHE_DIR), # on Kubeflow GKE/NFS \n",
    "    cachesubdir = dsl.PipelineParam('cachesubdir', value=\"datasets\"),\n",
    "    \n",
    "    ## -- Prepare Dataset Kubeflow Pipeline component parameters (with default values)\n",
    "    pathimgsubdir = dsl.PipelineParam('pathimgsubdir', value=\"train/\"),\n",
    "    pathtfrecords = dsl.PipelineParam('pathtfrecords', value=PATH_TO_TFRECORDS), # on Kubeflow GKE/NFS\n",
    "    \n",
    "    # earlystop param will be used in several pipelines components  \n",
    "    earlystop = dsl.PipelineParam('earlystop', value=10),\n",
    "    \n",
    "    ## -- Training Kubeflow Pipeline component (with default values)\n",
    "    pathtflogs = dsl.PipelineParam('pathtflogs', value=PATH_TO_TF_LOGS),\n",
    "    pathoutputs = dsl.PipelineParam('pathoutputs', value=PATH_TO_OUTPUTS),\n",
    "    pathcheckpoints = dsl.PipelineParam('pathcheckpoints', value=PATH_TO_CHECKPOINTS),\n",
    "    epochs = dsl.PipelineParam('epochs', value=\"1\"), \n",
    "    initialresize = dsl.PipelineParam('initialresize', value=\"286\"), \n",
    "    cropresize = dsl.PipelineParam('cropresize', value=\"256\"),\n",
    "    resizemethod = dsl.PipelineParam('resizemethod', value=\"1\"), \n",
    "    saveevery = dsl.PipelineParam('saveevery', value=\"1\")\n",
    "   \n",
    "):\n",
    "    \n",
    "       \n",
    "    # Passing pipeline parameters as operation arguments (Returns a dsl.ContainerOp class instance)\n",
    "    download_task = download_op(fname, origin, cachedir, cachesubdir) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    \n",
    "    # Single output value of previous pipeline component will be used in input of next pipeline component \n",
    "    preparation_task = preparation_op(download_task.output, pathimgsubdir, pathtfrecords, earlystop ) \\\n",
    "                                      .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                      persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                      .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    \n",
    "    # Single output value of previous pipeline component will be used in input of next pipeline component \n",
    "    training_task = training_op(preparation_task.output, pathtflogs, pathoutputs, pathcheckpoints, epochs,\n",
    "                                initialresize, cropresize, resizemethod, saveevery, earlystop) \\\n",
    "                                .add_volume(k8s_client.V1Volume(name='workdir', \n",
    "                                                                persistent_volume_claim=k8s_client.V1PersistentVolumeClaimVolumeSource(claim_name='nfs'))) \\\n",
    "                                .add_volume_mount(k8s_client.V1VolumeMount(mount_path=NFS_MOUNT, name='workdir'))\n",
    "    \n",
    "    # Allow to write Tensorboard logs on Google Storage\n",
    "    training_task.apply(gcp.use_gcp_secret('user-gcp-sa')) \n",
    "    \n",
    "    # To the training task on a GPU node\n",
    "    training_task.set_gpu_limit(1)   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Create/Reuse an *EXPERIMENT* and submit a Pipeline *RUN*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------- \n",
    "#            Compile the Pipeline \n",
    "#--------------------------------------------- \n",
    "pipeline_filename = pix2pix.__name__ + '.pipeline.tar.gz'\n",
    "compiler.Compiler().compile(pipeline_func=pix2pix, \n",
    "                            package_path=pipeline_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(NEW)** Create an instance of the Kubeflow Pipelines Client\n",
    "\n",
    "The only change to submit a remote execution on the Kubeflow cluster, is to pass additional parameters in instance creation  of the Kubeflow Pipelines.\n",
    "\n",
    "Just change this:\n",
    "```\n",
    "client = kfp.Client()\n",
    "```\n",
    "bythis:\n",
    "\n",
    "```\n",
    "client = kfp.Client(host=<YOUR_KUPEFLOW_PIPELINE_ENDPOINTS_URL>/pipeline ,\n",
    "                    client_id=>YOUR_CLIENT_ID)\n",
    "```                    \n",
    "                    \n",
    "*Et Voilà!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------- \n",
    "#  Create a Kubeflow Pipeline Experiment\n",
    "#--------------------------------------------- \n",
    "#---------------------------------------------------- \n",
    "EXPERIMENT_NAME = \"Next - Pix2Pix\"   ## Customize Name\n",
    "\n",
    "#--------------------------------------------- \n",
    "#         Create an instance of the \n",
    "#         Kubeflow Pipelines client\n",
    "#\n",
    "# IMPORTANT  : CUSTOMIZE this variable  \n",
    "#             with your own cluster info\n",
    "#\n",
    "#--------------------------------------------- \n",
    "client = kfp.Client(host=<YOUR_KUPEFLOW_PIPELINE_ENDPOINTS_URL>/pipeline ,\n",
    "                    client_id=>YOUR_CLIENT_ID)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"http://https://kubeflow.endpoints.nextatos-201903.cloud.goog/pipeline/#/runs/details/f790bb9c-5452-11e9-9810-42010a8a00d9\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    experiment = client.get_experiment(experiment_name=EXPERIMENT_NAME)\n",
    "except:\n",
    "    experiment = client.create_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "#-------------------------------------------------- \n",
    "#  Optional : Specify/Overwrite pipeline arguments \n",
    "#  values for execution or use default values)\n",
    "#-------------------------------------------------- \n",
    "arguments = {'epochs': 20, # Change to 200 for a full training\n",
    "             'initialresize' : 286,\n",
    "             'cropresize': 256,\n",
    "             'saveevery': 100,\n",
    "             'earlystop': 0  \n",
    "            }\n",
    "\n",
    "#-------------------------------------------------- \n",
    "#             Submit a pipeline run\n",
    "#--------------------------------------------------\n",
    "run_name = pix2pix.__name__ + ' remote run'\n",
    "#run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename)\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------\n",
    "#  Don't care about the environment thanks to GKE Autoscaling !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Submit another pipeline run using a GPU...and let's see what is happening on the GCP console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------- \n",
    "#  Optional : Specify/Overwrite pipeline arguments \n",
    "#  values for execution or use default values)\n",
    "#-------------------------------------------------- \n",
    "arguments = {'epochs': 20, # Change to 200 for a full training\n",
    "             'initialresize' : 286,\n",
    "             'cropresize': 256,\n",
    "             'saveevery': 100,\n",
    "             'earlystop': 0  \n",
    "            }\n",
    "\n",
    "#-------------------------------------------------- \n",
    "#             Submit a pipeline run\n",
    "#--------------------------------------------------\n",
    "run_name = pix2pix.__name__ + ' - Remote run 2 (with Autoscaling)'\n",
    "run_result = client.run_pipeline(experiment.id, run_name, pipeline_filename, arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
